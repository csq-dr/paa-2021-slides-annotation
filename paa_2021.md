# Slide 1: Title
_Presentation given at the [Population Association of America (PAA) Annual Meeting 2021](https://www.populationassociation.org/paa-2021/home)._

Hello, my name is Chuchu Wei from the University of Massachusetts Amherst. Iâ€™ll be presenting this joint work with [Leontine Alkema](https://leontinealkema.github.io/alkema_lab/). In this study, we try to answer the question of population proportion estimation with data being subject to potential misclassification error.

# Slide 2: Introduction

Our motivating question is how to estimate contraceptive prevalence (CP) for all countries in the world using self-reported usage data from national level surveys like demographic health surveys (DHS).

We focus on an exisiting approach used by the UN and FP2020 to do so, which is a Bayesian model called the family planning estimation model (FPEM) to estimate prevalence in all countries.

Here are two examples of observed self-reported usage of modern contraception over time in Ghana and Nepal, two countries at different stages of their contraceptive transition.

<!-- Here are two examples of observed self-reported usage of modern contraception over time in Burundi and Colombia, two countries at different stages of their contraceptive transition. -->

The circles refer to observed prevalence from surveys. The estimates from FPEM are added to the country plots, with solid lines referring to point estimates and dashed lines to 80% credible intervals.

# Slide 3: How is the data used in FPEM?

In FPEM, we distinguish between a process model and a data model. A process model is used to describe true prevalence and how it changes with time and allows for estimation and projections to years without data. In this talk, I will focus on the data model that describes how data related to true prevalence.

Let $y$ refer to observed mCPR and $\theta$ to the unknown true mCPR. We assume that the logit-transformed $y$'s are normally distributed with mean equal to logit-transformed $\theta$, and a variance that is the sum of sampling variance $\text{logit}.s^2$, and a non-sampling variance $\tau^2$. 

This data model is motivated by the fact that observed $y$ are obtained in a survey and thus subject to sampling error, and a typical assumption that data are subject to additional non-sampling error. No info of non-sampling error was available at the time FPEM was developed, hence NSE is estimated in FPEM and is around 10% for mCPR.

The graph on the right is the relationship between observed $y$ and estimates of $\theta$ based on the data model. We present this relationship as the Bayesian estimates when assuming a uniform prior $U(0,1)$ on $\theta$. So assuming any value for $\theta$ is equally likely.

In this graph of the data model assumption, non-sampling errors are assumed to increase uncertainty when prevalence is close to 0.5 but decrease to zero as prevalence is close to 0 or 1.

Is this kind of relationship reasonable? To assess that question, we considered external data on non-sampling errors in self-reported usage.

# Slide 4: What if we observed some non-sampling error?

In the previous slide, we have shown that there is no particular assumption of the non-sampling error $\tau$ in the current data model.  However, what if we have some evidence of the non-sampling error, for example, the non-sampling error in the form of misclassification error? 

There are two studies that collect more detailed follow-up data on contraceptive use among women interviewed in the DHS in Ghana and Nepal. Treating the post-survey findings as the truth and comparing them to the DHS results, we summarize misclassification error in terms of sensitivity and specificity, where - in this context - sensitivity of the reporting of modern use refers to the true proportion of modern users who reported themselves as such in the DHS while specificifity refers to the reporting of non-modern users who reported themselves as such. If there is no misclassification, both sensitivity and specificity equals to 1. In the two studies, we see sensitivity and specificity being less than 1 thus suggesting misclassification errors being present in the DHS. 

Given the findings of those studies, the question is, what does the relation between true prevalence and self-reported use, or in other wordds, the data model, look like with such evidence of non-sampling error?

# Slide 5: Visualization of the relation between true prevalence $\theta$ and self-reported use $y$ in presence of misclassification

With sensitivity $se$ and specificity $sp$, observed $y$ now has a linear relationship with true prevalence $\theta$. If $se, sp$ are indeed less than 1, the expected value of $y$ does not equal to $\theta$, as illustrated in these graphs based on the estimates of sensitivity and specificity for Ghana and Nepal. Here the uncertainty associated with true prevalence follows from the uncertainty in sensitivity and specificity. 

Comparing the relation between observed and true prevalence as observed in the post-surveys, based on $se, sp$ from Ghana and Nepal studies, to the one implied by the current FPEM data model, we see a clear mismatch between the two assumptions. Specifically, we see that in the current logit-normal data model, uncertainty introduced by non-sampling error does not capture the uncertainty implied by misclassification. This leads us to our research question

# Slide 6: How to estimate a population proportion if data are possibly subject to misclassification error? 

The conclusions in the context of estimating modern use are that, first, we have two small studies suggests self-reported mCPR is subject to misclassification. Second, the current data model assumption cannot capture the additional uncertainty implied by misclassification found in the two studies.

At first sight, the discrepancy may suggest an obvious next step to replace the current data model in FPEM with one that explicitly accounts for misclassification and the bias and uncertainty associated with it. But, we only have two studies in very specific settings that are not necessarily generalizable to other settings. Hence, we do not have enough external data to apply a bias adjustment to all other DHS data used in FPEM. 

What we \textbf{can} do is to update the data model to better reflect uncertainty associated with potential misclassification errors. 

# Slide 7: Proposed new data model based on a Normal-Laplace distribution

We propose a new data model that can better reflect uncertainty associated with potential misclassification errors using a Normal-Laplace distribution. We formulate our aims based on user-provided values for sensitivity, denoted by $se^a$, and specificity $sp^a$ which are meant to affect the uncertainty in the estimates but not the point estimates. More precisely, regardless of the assumed values for sensitivity and specificity, our first aim is that the estimate of true prevalence $\hat\theta$ still equals the observed $y$. Second, we want the 95% CI of $\theta$ to be determined by sampling error $s$ if there is no misclassification. Lastly and most importantly, we want the 95% CI of $\theta$ to be determined by $s$, assumed sensitivity $se^a$, and assumed specificity $sp^a$, with increased CIs if necessary based on assumed misclassification values. 

We find use the Normal-Laplace (NL) distribution to accomplish the aims. It is a four-parameter distribution from the convolution of Normal and Laplace distribution. We utilize the NL density as the normalized likelihood of $\theta$ as the new data model, and fixed the four parameters to meet our aims.

The right graphs shows two examples of NL distribution. 

# Slide 8: The visualization of the new data model
# Slide 9: 
# Slide 10: 
# Slide 11: 
