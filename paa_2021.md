# Slide 1: Title
_Presentation given at the [Population Association of America (PAA) Annual Meeting 2021](https://www.populationassociation.org/paa-2021/home)._

Hello, my name is Chuchu Wei from the University of Massachusetts Amherst. Iâ€™ll be presenting this joint work with [Leontine Alkema](https://leontinealkema.github.io/alkema_lab/). In this study, we try to answer the question of population proportion estimation with data being subject to potential misclassification error.

# Slide 2: Introduction

Our motivating question is how to estimate contraceptive prevalence (CP) for all countries in the world using self-reported usage data from national level surveys like demographic health surveys (DHS).

We focus on an exisiting approach used by the UN and FP2020 to do so, which is a Bayesian model called the family planning estimation model (FPEM) to estimate prevalence in all countries.

Here are two examples of observed self-reported usage of modern contraception over time in Ghana and Nepal, two countries at different stages of their contraceptive transition.

<!-- Here are two examples of observed self-reported usage of modern contraception over time in Burundi and Colombia, two countries at different stages of their contraceptive transition. -->

The circles refer to observed prevalence from surveys. The estimates from FPEM are added to the country plots, with solid lines referring to point estimates and dashed lines to 80% credible intervals.

# Slide 3: How is the data used in FPEM?

In FPEM, we distinguish between a process model and a data model. A process model is used to describe true prevalence and how it changes with time and allows for estimation and projections to years without data. In this talk, I will focus on the data model that describes how data related to true prevalence.

Let $y$ refer to observed mCPR and $\theta$ to the unknown true mCPR. We assume that the logit-transformed $y$'s are normally distributed with mean equal to logit-transformed $\theta$, and a variance that is the sum of sampling variance $\text{logit}.s^2$, and a non-sampling variance $\tau^2$. 

This data model is motivated by the fact that observed $y$ are obtained in a survey and thus subject to sampling error, and a typical assumption that data are subject to additional non-sampling error. No info of non-sampling error was available at the time FPEM was developed, hence NSE is estimated in FPEM and is around 10% for mCPR.

The graph on the right shows the relationship between observed $y$ and estimates of $\theta$ based on the data model. We present this relationship as the Bayesian estimates when assuming a uniform prior $U(0,1)$ on $\theta$. So assuming any value for $\theta$ is equally likely.

The graph shows that based on the logit-normal data model assumption, non-sampling errors are assumed to increase uncertainty when prevalence is close to 0.5 but decrease to zero as prevalence is close to 0 or 1.

Is this kind of relationship reasonable? To assess that question, we considered external data on non-sampling errors in self-reported usage.

# Slide 4: What if we observed some non-sampling error?

There are two studies that collect more detailed follow-up data on contraceptive use among women interviewed in the DHS in Ghana and Nepal. Treating the post-survey findings as the truth and comparing them to the DHS results, we summarize misclassification error in terms of sensitivity and specificity. In the context of this study, sensitivity of the reporting of modern use refers to the true proportion of modern users who reported themselves as such in the DHS. Specificity refers to the reporting of non-modern users who reported themselves as such. If there is no misclassification, both sensitivity and specificity equals to 1. However, in the two studies, we observe sensitivity and specificity being less than 1, which suggests potential misclassification errors in the DHS. 

Given the findings of those studies, the question is, what does the relation between true prevalence and self-reported use, or in other words, the data model, look like with such evidence of non-sampling error?

# Slide 5: Visualization of the relation between true prevalence $\theta$ and self-reported use $y$ in presence of misclassification

With sensitivity $se$ and specificity $sp$ less than 1, the expected value of observed prevalence $y$ now has a linear relationship with true prevalence $\theta$, as summarized in this simplified data model equation and illustrated in the two graphs here based on the estimates of sensitivity and specificity for Ghana and Nepal. Here the uncertainty associated with true prevalence follows from the uncertainty in sensitivity and specificity in each of the studies. 

Comparing the relation between observed and true prevalence as observed in the post-surveys, based on $se, sp$ from Ghana and Nepal studies, to the one implied by the current FPEM data model, we see a clear mismatch between the two assumptions. Specifically, we see that in the current logit-normal data model, uncertainty introduced by non-sampling error does not capture the uncertainty implied by misclassification. This leads us to our research question

# Slide 6: How to estimate a population proportion if data are possibly subject to misclassification error? 

The conclusions in the context of estimating modern use are that, first, we have two small studies suggests self-reported mCPR is subject to misclassification. Second, the current data model assumption cannot capture the additional uncertainty implied by misclassification found in the two studies.

At first sight, the discrepancy may suggest an obvious next step to replace the current data model in FPEM with one that explicitly accounts for misclassification and the bias and uncertainty associated with it. But, we only have two studies in very specific settings that are not necessarily generalizable to other settings. Hence, we do not have enough external data to apply a bias adjustment to all other DHS data used in FPEM. 

What we __can__ do is to update the data model to better reflect uncertainty associated with potential misclassification errors. 

# Slide 7: Proposed new data model based on a Normal-Laplace distribution

We propose a new data model that can better reflect uncertainty associated with potential misclassification errors using a Normal-Laplace distribution. We formulate our aims based on user-provided values for sensitivity, denoted by $se^a$, and specificity $sp^a$ which are meant to affect the uncertainty in the estimates but not the point estimates. More precisely, regardless of the assumed values for sensitivity and specificity, our first aim is that the estimate of true prevalence $\hat\theta$ still equals the observed $y$. Second, we want the 95% CI of $\theta$ to be determined by sampling error $s$ if there is no misclassification. Lastly and most importantly, we want the 95% CI of $\theta$ to be determined by $s$, assumed sensitivity $se^a$, and assumed specificity $sp^a$, with increased CIs if necessary based on assumed misclassification values. 

We use the Normal-Laplace (NL) distribution to accomplish the aims. It is a four-parameter distribution from the convolution of Normal and Laplace distribution. We utilize the NL density as the normalized likelihood of $\theta$ as the new data model, and fixed the four parameters to meet our aims.

# Slide 8: Illustration of the new data model

One reason for using the NL distribution is that this distribution allows for asymmetric credible intervals for the posterior such that 95% CI of point estimates can cover the potential biased prevalence within the uncertainty. 

The graph on the right shows relationship between observed $y$ and estimates of $\theta$ for NL model vs the true data model.  Here we assume sampling error $s = 0.01$ and $se = 0.9, sp = 1$ for both models. We see that the NL model always includes the estimates based on the true data model, while the point estimates from NL model align to the line of $\hat\theta = y$ as per our aims.

# Slide 9: Simulation study

Before we apply this NL model to FPEM, we conduct a simulation to study the effect of misclassification error on estimation with some generated data. In the simulation, we fixed the true $\theta$ to be 0.3, 0.5, 0.7, and use various combinations of true misclassification and assumed misclassification, corresponding to situations that we make correct assumptions regarding the extent of misclassification as well as settings where we over- or underestimate it. 

We generate 100 data sets for each setting and then obtain posterior estimates and 95% credible intervals based on model fitting using the logit-normal and NL data models.

This simulation study results in findings as expected: first, point estimates are comparable between the logit-normal and NL model regardless of the bias. Second, 95% credible intervals from the NL model are conservative, meaning that they are larger than needed, when assumed misclassification exceeds true misclassification. Lastly, the NL model improves upon logit-normal model in terms of coverage of 95% credible interval when misclassification is present and accounted for.

# Slide 10: New data model in FPEM

Simulation results suggest improvements for the NL model as compared to the logit-normal model so we implement the NL model in FPEM. Currently we use the NL model in FPEM for DHS data points only given that the evidence of misclassification is for DHS data collection only. We use two sets of misclassification assumptions, with the first set informed by findings from the Ghana study and the second one by the Nepal follow-up study.

In the FPEM fitting, we find that the effect of changes from logit-normal to NL model is variable, for example, we don't see much changes in Nepal with the assumption $se^a = 0.9, sp^a = 1$. This is due to FPEM estimates being not only informed by DHS data but also informed by the process model and additional non-DHS data that still has the logit-normal model.

# Slide 11: New data model in FPEM (ctd)
When there are differences between the fits on the logit-normal and NL data models, we find that estimated bounds vary in the direction that's expected based on the assumed values for sens and spec. For example, this figure shows the comparison for Colombia with the NL model in red with $se^a = 0.9, sp^a = 1$. We see that the NL-based upperbound has increased as expected. We also find something that we did not expect, which is that point estimates and lower bounds have increased. We are currently investigating what explains this effect. 

# Slide 12: Summary
In summary, what we investigated in this paper is how to estimate a population proportion if data are possibly subject to misclassification error, motivated by reported evidence on misclassification in self-reported modern contraceptive use that is not generalizable to all settings. We propose a new normal-laplace data model to account for increased asymmetric uncertainty associated with potential misclassification errors while still producing point estimates as if there is no misclassification. We carry out a simulation study that shows improvement in coverage of credible intervals when data are subject to misclassification and implemented the new proposed in FPEM. Our current ongoing work is to better understand the effect of the change in data model from logit-normal to the normal-laplace in FPEM. 

We would like to thank the organizer again for the opportunity to present our work in this session and we welcome comments and feedback. 
