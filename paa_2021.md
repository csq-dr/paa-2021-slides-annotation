# Slide 1: Title
Hello, my name is Chuchu Wei from the University of Massachusetts Amherst. Today Iâ€™ll be presenting this joint work with my advisor [Leontine Alkema](https://leontinealkema.github.io/alkema_lab/). 

In this study, we try to answer the question of population proportion estimation with data being subject to potential misclassification error, and present a case based on self-reported usage.

# Slide 2: Introduction

Our motivating question is how to estimate modern contraceptive prevalence rate (mCPR) for all countries in the world using self-reported usage data from national level surveys like demographic health surveys (DHS).

We focus on an existing approach used by the united nation population division (UNPD) and FP2020 project to do so, which is a Bayesian model called the family planning estimation model (FPEM) to estimate prevalence in countries, sub-regions, regions and worldwide with country-year self-reported survey data from multiple data sources.

Here are two examples of observed self-reported usage of modern contraception over time in Colombia and Nepal, two countries at different stages of their contraceptive transition.

The circles refer to observed prevalence from surveys. The estimates from FPEM are added to the country plots, with solid lines referring to point estimates and dashed lines to 80% credible intervals.

# Slide 3: Survey data on self-reported usage

In FPEM, we use the observed modern and traditional contraceptive prevalence rates and unmet needs as the model data inputs. These rates are aggregated proportions from commonly available data sources like demographic health surveys (DHS), multiple indicator cluster surveys (MICS), national surveys, etc. 

Surveys like DHS reports sampling error associated with mCPR/tCPR as well. However, no information on non-sampling error are included in survey data collection yet. 

In the next slides, I will talk about how the data is used and how the sampling/non-sampling error is handled in FPEM.

# Slide 4: How is the data used in FPEM?

In FPEM, we distinguish between a process model and a data model. 
A process model is used to describe true prevalence <!-- and how it changes with time and allows for estimation and projections to years without data -->. 

The data model is used to introduce data inputs to the model and update parameters in the process model.

In this talk, I will focus on the data model that describes how data related to true prevalence.

Here we denote $y$ refer to observed mCPR and $\theta$ to the unknown true mCPR, and assume the logit-transformed $y$'s are normally distributed with mean equal to logit-transformed $\theta$. Variance of this normal is the sum of transformed sampling variance $\text{logit}.s^2$, and a non-sampling variance $\tau^2$. 

This data model is motivated by the fact that observed $y$ are obtained in a survey and thus subject to sampling error, and a typical assumption that data are subject to additional non-sampling error estimated to be around 10%.

<!-- . No info of non-sampling error was available at the time FPEM was developed, hence NSE is estimated in FPEM and is around 10% for mCPR. -->

The graph on the right shows the relationship between observed $y$ and estimates of $\theta$ based on the data model. We present this relationship as the Bayesian estimates when assuming a uniform prior $U(0, 1)$ on $\theta$. This graph tells that, based on the logit-normal data model assumption, non-sampling errors are assumed to increase uncertainty when prevalence is close to 0.5, but decrease to zero as prevalence is close to 0 or 1.

Is this kind of relationship reasonable? To assess that question, we considered external data on non-sampling errors in self-reported usage.

We only find case studies that includes follow-up data with non-sampling info for specific countries.

# Slide 5: Data on non-sampling error in self-reported modern contraceptive use

There are two studies that collect more detailed follow-up data on contraceptive use among women interviewed in the DHS in Ghana and Nepal. 

Treating the post-survey findings as the truth and comparing them to the DHS results, we summarize misclassification error in terms of sensitivity and specificity. 

In the context of this study, sensitivity of the reporting of modern use refers to the true proportion of modern users who reported themselves as such in the DHS. Specificity refers to the reporting of non-modern users who reported themselves as such. If there is no misclassification, both sensitivity and specificity equals to 1. However, in the two studies, we observe sensitivity and specificity being less than 1, which suggests potential misclassification errors in the DHS. 

Given the findings of those studies, the question is, what does the relation between true prevalence and self-reported use, or in other words, the data model, look like with such evidence of non-sampling error?

# Slide 6: Visualization of the relation between true prevalence $\theta$ and self-reported use $y$ in presence of misclassification

With sensitivity $se$ and specificity $sp$ less than 1, the expected value of observed prevalence $y$ now has a linear relationship with true prevalence $\theta$, as summarized in this simplified data model equation and illustrated in the two graphs here based on the estimates of sensitivity and specificity for Ghana and Nepal. Here the uncertainty associated with true prevalence follows from the uncertainty in sensitivity and specificity in each of the studies. 

Comparing the relation between observed and true prevalence as observed in the post-surveys, based on $se, sp$ from Ghana and Nepal studies, to the one implied by the current FPEM data model, we see a clear mismatch between the two assumptions. Specifically, we see that in the current logit-normal data model, uncertainty introduced by non-sampling error does not capture the uncertainty implied by misclassification. This leads us to our research question -

# Slide 7: How to estimate a population proportion if data are possibly subject to misclassification error? 

The conclusions in the context of estimating modern use are that, first, we have two small studies suggests self-reported mCPR is subject to misclassification. Second, the current data model assumption cannot capture the additional uncertainty implied by misclassification found in the two studies.

At first sight, the discrepancy may suggest an obvious next step to replace the current data model in FPEM with one that explicitly accounts for misclassification and the bias and uncertainty associated with it. But, we only have two studies in very specific settings that are not necessarily generalizable to other settings. Hence, we do not have enough external data to apply a bias adjustment to all other DHS data used in FPEM. 

What we __can__ do is to __update the data model__ to better reflect uncertainty associated with potential misclassification errors. 

# Slide 8: Proposed new data model

We are going to propose a new data model that can better reflect uncertainty associated with potential misclassification errors. 

We formulate our aims based on user-provided values for sensitivity, denoted by $se^a$, and specificity $sp^a$ which are meant to affect the uncertainty in the estimates but not the point estimates. 

More precisely, regardless of the assumed values for sensitivity and specificity, our first aim is that the estimate of true prevalence $\hat\theta$ still equals the observed $y$. Second, we want the 95% CI of $\theta$ to be determined by sampling error $s$ if there is no misclassification. Lastly and most importantly, we want the 95% CI of $\theta$ to be determined by $s$, assumed sensitivity $se^a$, and assumed specificity $sp^a$, with increased CIs if necessary based on assumed misclassification values. 

# Slide 9: Model specification: Normal-Laplace distribution

We use the Normal-Laplace (NL) distribution to accomplish the aims. 

It is a four-parameter distribution from the convolution of Normal and Laplace distribution. We utilize the NL density as the normalized likelihood of $\theta$ as the new data model, and fixed the four parameters to meet our aims.

Here are some examples of the normal-laplace distribution. In this example, we plot symmetric NL distribution in red dashed lines, and two asymmetric NL distribution in blue and green lines. The symmetric and asymmetric NL distributions share the same normal kernel, while the skewness is controlled independently by the Laplace components. 

Note here that the center of NL distribution is affected by Laplace components. We can clearly see that with a larger $\sigma$ on the right hand side peaks with asymmetric NL distributions shifted.

# Slide 10: More examples

Here are more examples of the NL distribution. On the left hand side it's comparison between NL and normal kernel when the Laplace component is ignorable.

On the right hand side the example shows fixed skewness with varying level of the normal kernel, i.e. the skewness is independent from the center.

# Slide 11: Illustration of the new data model

<!-- One reason for using the NL distribution is that this distribution allows for asymmetric credible intervals for the posterior such that 95% CI of point estimates can cover the potential biased prevalence within the uncertainty.  -->

Here is a illustration of the new data model. The graph here shows relationship between observed $y$ and estimates of $\theta$ for NL model vs the true data model. 
We assume sampling error $s = 0.01$ and $se = 0.9, sp = 1$ for both models. We see that the NL model always includes the estimates based on the true data model, while the point estimates from NL model align to the line of $\hat\theta = y$ as per our aims.

# Slide 12: Simulation study

Before we apply this NL model to FPEM, we conduct a simulation to study the effect of misclassification error on estimation with some generated data. In the simulation, we fixed the true $\theta$ to be 0.3, 0.5, 0.7, and use various combinations of true misclassification and assumed misclassification, corresponding to situations that we make correct assumptions regarding the extent of misclassification as well as settings where we over- or under-estimate it. 

<!-- We generate 100 data sets for each setting and then obtain posterior estimates and 95% credible intervals based on model fitting using the logit-normal and NL data models. -->

This simulation study results in findings as expected: first, point estimates are comparable between the logit-normal and NL model regardless of the bias. Second, 95% credible intervals from the NL model are conservative, meaning that they are larger than needed, when assumed misclassification exceeds true misclassification. Lastly, the NL model improves upon logit-normal model in terms of coverage of 95% credible interval when misclassification is present and accounted for.

# Slide 13: Estimate FP indicators in local FPEM

Next, we implement the NL model in local FPEM as simulation results suggest improvements for the NL model compared with the logit-normal model. 

What is local FPEM? It is a scale-down implementation of global FPEM.

Parameters in the process model are fixed from the most recent outcomes of global FPEM => no updating of the process model

In the FPEM fitting, we find that the effect of changes from logit-normal to NL model is variable, for example, we don't see much changes in Nepal with the assumption $se^a = 0.9, sp^a = 1$. This is due to FPEM estimates being not only informed by DHS data but also informed by the process model and additional non-DHS data that still has the logit-normal model.

# Slide 14: Data models for mCPR
When there are differences between the fits on the logit-normal and NL data models, we find that estimated bounds vary in the direction that's expected based on the assumed values for sens and spec. 

# Slide 15-17: Results

The results in Nepal is a typical example such that the estimated trends change little and data-driven depending on different misclassification assumptions.

Results in Ghana shows wider CI at lower level of mCPR.

Results in Niger shows some unusual changes when combining observations that contribute an likelihood function. We see a larger change in the estimated trends and much wider CI, esp. at year 2017 with two data points are on different levels (~ 0.1 difference)

# Slide 18: Joint likelihood changes in Niger

Point estimates can be affected based on the joint likelihood taking account of the varying levels of (asymmetric) uncertainty in the individual likelihood functions.

# Slide 19: Summary
In summary, what we investigated in this paper is how to estimate a population proportion if data are possibly subject to misclassification error, motivated by reported evidence on misclassification in self-reported modern contraceptive use that is not generalizable to all settings. We propose a new normal-laplace data model to account for increased asymmetric uncertainty associated with potential misclassification errors while still producing point estimates as if there is no misclassification. We carry out a simulation study that shows improvement in coverage of credible intervals when data are subject to misclassification and implemented the new proposed in local FPEM. The comparison of old/new data model in local FPEM with fixed global parameters show that-

